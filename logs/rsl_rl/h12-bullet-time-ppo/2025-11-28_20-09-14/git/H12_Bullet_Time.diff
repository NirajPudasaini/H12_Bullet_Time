--- git status ---
On branch main
Your branch is up to date with 'origin/main'.

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   h12_bullet_time/source/h12_bullet_time/h12_bullet_time/tasks/manager_based/h12_bullet_time/h12_bullet_time_env_cfg_curriculum_phase.py
	modified:   h12_bullet_time/source/h12_bullet_time/h12_bullet_time/tasks/manager_based/h12_bullet_time/mdp/events.py
	modified:   h12_bullet_time/source/h12_bullet_time/h12_bullet_time/tasks/manager_based/h12_bullet_time/mdp/rewards.py
	modified:   logs/rsl_rl/h12-bullet-time-ppo/2025-11-24_19-42-45/exported/policy.onnx
	modified:   logs/rsl_rl/h12-bullet-time-ppo/2025-11-24_19-42-45/exported/policy.pt

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	logs/rsl_rl/h12-bullet-time-ppo/2025-11-28_16-14-43/
	logs/rsl_rl/h12-bullet-time-ppo/2025-11-28_16-27-48/
	logs/rsl_rl/h12-bullet-time-ppo/2025-11-28_16-34-35/
	logs/rsl_rl/h12-bullet-time-ppo/2025-11-28_16-39-50/
	logs/rsl_rl/h12-bullet-time-ppo/2025-11-28_16-46-27/
	logs/rsl_rl/h12-bullet-time-ppo/2025-11-28_16-51-18/
	logs/rsl_rl/h12-bullet-time-ppo/2025-11-28_17-04-43/
	logs/rsl_rl/h12-bullet-time-ppo/2025-11-28_17-13-52/
	logs/rsl_rl/h12-bullet-time-ppo/2025-11-28_17-24-38/
	logs/rsl_rl/h12-bullet-time-ppo/2025-11-28_19-32-18/
	logs/rsl_rl/h12-bullet-time-ppo/2025-11-28_19-42-59/
	logs/rsl_rl/h12-bullet-time-ppo/2025-11-28_19-51-19/
	logs/rsl_rl/h12-bullet-time-ppo/2025-11-28_19-58-18/
	logs/rsl_rl/h12-bullet-time-ppo/2025-11-28_20-01-39/
	logs/rsl_rl/h12-bullet-time-ppo/2025-11-28_20-09-14/
	outputs/2025-11-27/
	outputs/2025-11-28/

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/h12_bullet_time/source/h12_bullet_time/h12_bullet_time/tasks/manager_based/h12_bullet_time/h12_bullet_time_env_cfg_curriculum_phase.py b/h12_bullet_time/source/h12_bullet_time/h12_bullet_time/tasks/manager_based/h12_bullet_time/h12_bullet_time_env_cfg_curriculum_phase.py
index 3ad831e..9674228 100644
--- a/h12_bullet_time/source/h12_bullet_time/h12_bullet_time/tasks/manager_based/h12_bullet_time/h12_bullet_time_env_cfg_curriculum_phase.py
+++ b/h12_bullet_time/source/h12_bullet_time/h12_bullet_time/tasks/manager_based/h12_bullet_time/h12_bullet_time_env_cfg_curriculum_phase.py
@@ -226,7 +226,7 @@ class RewardsCfg:
             "asset_cfg": SceneEntityCfg("robot"),
             "projectile_name": "Projectile",
             "max_distance": 3.0,
-            "penalty_scale": -10.0,  # Strong negative penalty when close
+            "penalty_scale": -30.0,  # Strong negative penalty when close
         },
     )
 
diff --git a/h12_bullet_time/source/h12_bullet_time/h12_bullet_time/tasks/manager_based/h12_bullet_time/mdp/events.py b/h12_bullet_time/source/h12_bullet_time/h12_bullet_time/tasks/manager_based/h12_bullet_time/mdp/events.py
index 109f9d0..d2818de 100644
--- a/h12_bullet_time/source/h12_bullet_time/h12_bullet_time/tasks/manager_based/h12_bullet_time/mdp/events.py
+++ b/h12_bullet_time/source/h12_bullet_time/h12_bullet_time/tasks/manager_based/h12_bullet_time/mdp/events.py
@@ -20,10 +20,12 @@ def launch_projectile(
 
     # Parameters (defaults chosen to spawn relative to torso link)
     spawn_distance = 1.5
-    height_offset = 1.0  # 30 cm above the specified robot link
+    height_offset = 0.8
     throw_speed = 5.0
     offset_range_x = 0.2
-    offset_range_z = 0.2
+    offset_range_z = 0.1
+    # Randomization: allow +/-20% variation on spawn_distance and height_offset per env
+    var_frac = 0.2  # 20%
 
     # Get projectile and robot from scene
     proj = env.scene[asset_cfg.name]
@@ -52,11 +54,18 @@ def launch_projectile(
     x_offset = torch.rand((n,), device=device, dtype=torch.float32) * 2 * offset_range_x - offset_range_x
     z_offset = torch.rand((n,), device=device, dtype=torch.float32) * 2 * offset_range_z - offset_range_z
 
+    # Per-env random scaling in [1-0.2, 1+0.2]
+    sd_scale = 1.0 + (torch.rand((n,), device=device, dtype=torch.float32) * 2.0 * var_frac - var_frac)
+    ho_scale = 1.0 + (torch.rand((n,), device=device, dtype=torch.float32) * 2.0 * var_frac - var_frac)
+    spawn_distance_per_env = spawn_distance * sd_scale
+    height_offset_per_env = height_offset * ho_scale
+
     # Spawn position: in front of link and height offset above link
     spawn_pos = torch.zeros((n, 3), device=device, dtype=torch.float32)
-    spawn_pos[:, 0] = center[:, 0] + spawn_distance + x_offset
+    # apply per-env spawn distance and height offset with random +/-20% variation
+    spawn_pos[:, 0] = center[:, 0] + spawn_distance_per_env + x_offset
     spawn_pos[:, 1] = center[:, 1]
-    spawn_pos[:, 2] = center[:, 2] + height_offset + z_offset
+    spawn_pos[:, 2] = center[:, 2] + height_offset_per_env + z_offset
 
     # Identity quaternion
     quats = torch.zeros((n, 4), device=device, dtype=torch.float32)
diff --git a/h12_bullet_time/source/h12_bullet_time/h12_bullet_time/tasks/manager_based/h12_bullet_time/mdp/rewards.py b/h12_bullet_time/source/h12_bullet_time/h12_bullet_time/tasks/manager_based/h12_bullet_time/mdp/rewards.py
index 64a1b58..96074c5 100644
--- a/h12_bullet_time/source/h12_bullet_time/h12_bullet_time/tasks/manager_based/h12_bullet_time/mdp/rewards.py
+++ b/h12_bullet_time/source/h12_bullet_time/h12_bullet_time/tasks/manager_based/h12_bullet_time/mdp/rewards.py
@@ -19,6 +19,7 @@ __all__ = [
     "projectile_hit_penalty",
     "projectile_proximity_penalty",
     "projectile_distance",
+    "projectile_contact_penalty",
     "torso_pitch_curriculum",
 ]
 
@@ -118,6 +119,60 @@ def projectile_hit_penalty(
     return reward
 
 
+def projectile_contact_penalty(
+    env: ManagerBasedRLEnv,
+    asset_cfg: SceneEntityCfg,
+    projectile_name: str = "Projectile",
+    contact_threshold: float = 0.05,
+    penalty: float = -500.0,
+) -> torch.Tensor:
+    """High-magnitude penalty when projectile is in contact (within threshold) with any robot body/link.
+
+    This function is reactive (checks proximity) but designed to produce a large negative
+    scalar that discourages the agent from making contact with the projectile.
+    """
+    # Get robot body positions
+    robot: Articulation = env.scene[asset_cfg.name]
+    try:
+        robot_body_positions = robot.data.body_pos_w  # (num_envs, num_bodies, 3)
+    except Exception:
+        # If body positions are not available, return zeros
+        return torch.zeros(env.num_envs, dtype=torch.float32, device=env.device)
+
+    # Get projectile
+    scene_names = list(env.scene.keys())
+    candidates = [] if projectile_name is None else [projectile_name]
+    if projectile_name is None:
+        for n in scene_names:
+            if "projectile" in n.lower() or "obstacle" in n.lower():
+                candidates.append(n)
+
+    if len(candidates) == 0:
+        return torch.zeros(env.num_envs, dtype=torch.float32, device=env.device)
+
+    penalty_tensor = torch.zeros(env.num_envs, dtype=torch.float32, device=env.device)
+
+    for name in candidates:
+        obj = env.scene[name]
+        try:
+            proj_pos = obj.data.root_pos_w  # (num_envs, 3)
+        except Exception:
+            try:
+                proj_pos = obj.data.body_pos_w[:, 0, :]
+            except Exception:
+                continue
+
+        # Compute distances to all robot bodies
+        distances = torch.norm(robot_body_positions - proj_pos.unsqueeze(1), dim=-1)  # (num_envs, num_bodies)
+        min_dist = distances.min(dim=1)[0]
+
+        hit_mask = min_dist < float(contact_threshold)
+        if hit_mask.any():
+            penalty_tensor[hit_mask] = float(penalty)
+
+    return penalty_tensor
+
+
 def projectile_proximity_penalty(
     env: ManagerBasedRLEnv,
     asset_cfg: SceneEntityCfg,
@@ -126,11 +181,9 @@ def projectile_proximity_penalty(
     penalty_scale: float = -1.0,
     approach_gain: float = 2.0,
 ) -> torch.Tensor:
-
     # Get robot
     robot: Articulation = env.scene[asset_cfg.name]
-    robot_pos = robot.data.root_pos_w[:, :3]  # shape: (num_envs, 3)
-    
+
     # Get projectile
     try:
         projectile = env.scene[projectile_name]
@@ -138,42 +191,86 @@ def projectile_proximity_penalty(
     except (KeyError, AttributeError):
         # Projectile not found, no penalty
         return torch.zeros(env.num_envs, dtype=torch.float32, device=env.device)
-    
-    # Compute distance from projectile to robot base
-    rel = proj_pos - robot_pos
-    distance = torch.norm(rel, dim=-1)  # shape: (num_envs,)
 
-    # Compute approach speed: projection of relative velocity onto relative vector
-    # proj_lin_vel: (num_envs, 3)
-    proj_lin_vel = projectile.data.root_lin_vel_w
-    # robot base linear velocity (world frame)
+    # Build a list of robot link positions to consider for proximity checks.
+    # Include base/root plus important upper-body links so the robot fully avoids obstacles.
+    positions_list = []
+    # root/base position (index 0)
     try:
-        robot_lin_vel = robot.data.root_lin_vel_w
+        root_pos = robot.data.root_pos_w[:, :3]
+        positions_list.append(root_pos.unsqueeze(1))  # (num_envs, 1, 3)
     except Exception:
-        robot_lin_vel = torch.zeros_like(proj_lin_vel)
-
-    # Relative velocity of projectile w.r.t robot base
-    rel_vel = proj_lin_vel - robot_lin_vel  # (num_envs, 3)
-
-    eps = 1e-6
-    # unit direction from robot -> projectile: avoid division by zero
-    dir_unit = rel / (distance.unsqueeze(-1) + eps)
-    # approach_speed = -dot(rel_vel, dir_unit) so positive when projectile moves toward robot
-    approach_speed = -torch.sum(rel_vel * dir_unit, dim=-1)  # (num_envs,)
-    approach_speed_clamped = torch.clamp(approach_speed, min=0.0)
-
-    # Base linear penalty: ramps from 0 at max_distance to penalty_scale at distance=0
-    base_penalty = float(penalty_scale) * (1.0 - distance / float(max_distance))
-    base_penalty = torch.clamp(base_penalty, min=float(penalty_scale), max=0.0)
-
-    # Boost penalty when projectile is approaching: factor = 1 + approach_gain * (approach_speed / (1 + approach_speed))
-    # This keeps the boost bounded while growing with approach speed.
-    approach_factor = 1.0 + float(approach_gain) * (approach_speed_clamped / (1.0 + approach_speed_clamped))
-
-    penalty = base_penalty * approach_factor
+        # Fallback to zeros if unavailable
+        positions_list.append(torch.zeros((env.num_envs, 1, 3), device=env.device, dtype=torch.float32))
+
+    # Preferred link names to check (as requested by user)
+    link_names_to_check = [
+        "left_elbow_link",
+        "right_elbow_link",
+        "left_shoulder_yaw_link",
+        "right_shoulder_yaw_link",
+        # Lidar mounted on torso (protect sensor)
+        "lidar_link",
+    ]
+
+    # Robot may expose body/link names via "body_names" attribute
+    body_names = []
+    try:
+        body_names = list(robot.body_names)
+    except Exception:
+        body_names = []
+
+    # Track which appended index corresponds to which link name so we can
+    # apply link-specific amplifications (e.g., for `lidar_link`).
+    link_indices: dict = {}
+
+    # For each requested link, if present, append its world position
+    for ln in link_names_to_check:
+        if ln in body_names:
+            idx = body_names.index(ln)
+            link_pos = robot.data.body_pos_w[:, idx, :]
+            # current index in concatenated positions will be len(positions_list)
+            link_indices[ln] = len(positions_list)
+            positions_list.append(link_pos.unsqueeze(1))
+
+    # Concatenate positions -> (num_envs, n_links, 3)
+    positions = torch.cat(positions_list, dim=1)
+
+    # Compute relative vectors from each considered link to projectile: (num_envs, n_links, 3)
+    rel = proj_pos.unsqueeze(1) - positions
+    distances = torch.norm(rel, dim=-1)  # (num_envs, n_links)
+
+    # Base penalty per link: purely distance-based (no approach-speed term)
+    # Linear ramp from 0 at max_distance to penalty_scale at distance=0
+    base_penalty_each = float(penalty_scale) * (1.0 - distances / float(max_distance))
+    base_penalty_each = torch.clamp(base_penalty_each, min=float(penalty_scale), max=0.0)
+
+    # Zero out penalties beyond max_distance per link
+    penalty_each = torch.where(distances >= float(max_distance), torch.zeros_like(base_penalty_each), base_penalty_each)
+
+    # Amplify penalty for lidar_link if present: we care strongly about the lidar
+    # being hit or closely approached (sensor protection). Multiply the per-link
+    # penalty by a factor so that close approaches to `lidar_link` are punished
+    # more heavily than other links. This only affects that link's penalty;
+    # the overall penalty still uses the minimum-distance link.
+    try:
+        if "lidar_link" in link_indices:
+            lidar_idx = link_indices["lidar_link"]
+            # Make lidar penalty more severe. Factor selected empirically â€”
+            # increase if you want even stronger protection.
+            lidar_factor = 3.0
+            lidar_pen = penalty_each[:, lidar_idx] * float(lidar_factor)
+            # Clamp to penalty_scale min (can't exceed the configured worst penalty)
+            lidar_pen = torch.clamp(lidar_pen, min=float(penalty_scale), max=0.0)
+            penalty_each[:, lidar_idx] = lidar_pen
+    except Exception:
+        # If anything goes wrong with link indexing, fall back silently.
+        pass
 
-    # For distances beyond max_distance, ensure penalty is zero
-    penalty = torch.where(distance >= float(max_distance), torch.zeros_like(penalty), penalty)
+    # Select the minimum distance link per environment (the one that matters most)
+    min_idx = distances.argmin(dim=1)  # (num_envs,)
+    batch_idx = torch.arange(env.num_envs, device=env.device)
+    penalty = penalty_each[batch_idx, min_idx]
 
     return penalty
 
diff --git a/logs/rsl_rl/h12-bullet-time-ppo/2025-11-24_19-42-45/exported/policy.onnx b/logs/rsl_rl/h12-bullet-time-ppo/2025-11-24_19-42-45/exported/policy.onnx
index 03704cd..7a22a3a 100644
Binary files a/logs/rsl_rl/h12-bullet-time-ppo/2025-11-24_19-42-45/exported/policy.onnx and b/logs/rsl_rl/h12-bullet-time-ppo/2025-11-24_19-42-45/exported/policy.onnx differ
diff --git a/logs/rsl_rl/h12-bullet-time-ppo/2025-11-24_19-42-45/exported/policy.pt b/logs/rsl_rl/h12-bullet-time-ppo/2025-11-24_19-42-45/exported/policy.pt
index 4928468..2990722 100644
Binary files a/logs/rsl_rl/h12-bullet-time-ppo/2025-11-24_19-42-45/exported/policy.pt and b/logs/rsl_rl/h12-bullet-time-ppo/2025-11-24_19-42-45/exported/policy.pt differ